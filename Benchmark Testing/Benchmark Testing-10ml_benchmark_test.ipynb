{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d7dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# advanced_ml_benchmark.py\n",
    "# Purpose:\n",
    "# 1. Train classic ML models using the same train/test splits as the 1D-CNN model.\n",
    "# 2. Evaluate 10 classical ML models on the internal test set and an external CRLM validation set.\n",
    "# 3. Save and visualize comparative results for both datasets.\n",
    "\n",
    "print(\"--- Starting advanced machine learning benchmark ---\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad7bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required models and tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a13193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 0: Global settings and path definitions ---\n",
    "print(\"\\n--- Step 0: Global settings and path definitions ---\")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Plot fonts (adjust if these fonts are not installed on your system)\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Base directories (edit to match your environment)\n",
    "BASE_DIR = r\"D:\\结直肠癌肝转移Biomarker 诊断\\新的策略\\Autoencoder\"\n",
    "TRAIN_DATA_DIR = BASE_DIR\n",
    "VALIDATION_DATA_DIR = os.path.join(BASE_DIR, \"validation_datasets\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"advanced_ml_benchmark_results\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Input files (edit names if needed)\n",
    "EXPRESSION_FILE = os.path.join(TRAIN_DATA_DIR, \"expression_data_combat_corrected.csv\")\n",
    "METADATA_FILE = os.path.join(TRAIN_DATA_DIR, \"metadata_combined.csv\")\n",
    "FUNC_GENES_FILE = os.path.join(TRAIN_DATA_DIR, \"functional_genes_620.txt\")\n",
    "EXTERNAL_VALIDATION_FILE = os.path.join(VALIDATION_DATA_DIR, \"dat_crlm.csv\")\n",
    "\n",
    "print(f\"Results will be saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8eee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Load and prepare training and internal test data ---\n",
    "print(\"\\n--- Step 1: Load and prepare training and internal test data ---\")\n",
    "try:\n",
    "    expression_data = pd.read_csv(EXPRESSION_FILE, index_col=0)\n",
    "    metadata = pd.read_csv(METADATA_FILE, index_col=0)\n",
    "    with open(FUNC_GENES_FILE, 'r', encoding='utf-8') as f:\n",
    "        functional_genes = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    print(\"✅ Required training files loaded successfully\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Error: training file not found - {e}\")\n",
    "    raise\n",
    "\n",
    "# Filter functional genes present in the expression matrix\n",
    "available_functional_genes = [gene for gene in functional_genes if gene in expression_data.columns]\n",
    "if len(available_functional_genes) == 0:\n",
    "    raise ValueError(\"No functional genes found in expression matrix. Check FUNC_GENES_FILE and EXPRESSION_FILE.\")\n",
    "\n",
    "X = expression_data[available_functional_genes]\n",
    "y_raw = metadata.reindex(X.index)['group']\n",
    "\n",
    "# Encode labels (metastasis=1, primary=0)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_raw.astype(str))\n",
    "print(f\"Label encoding mapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "\n",
    "# IMPORTANT: use the same data split strategy as the CNN model\n",
    "X_train, X_test_internal, y_train, y_test_internal = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "print(f\"Train/internal test split: {len(X_train)} training samples, {len(X_test_internal)} internal test samples\")\n",
    "\n",
    "# Standardize features: fit on training set, apply to test sets\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_internal_scaled = scaler.transform(X_test_internal)\n",
    "print(\"✅ Training and internal test data are ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05355304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Load and prepare external validation data ---\n",
    "print(\"\\n--- Step 2: Load and prepare external validation data ---\")\n",
    "try:\n",
    "    dat_crlm = pd.read_csv(EXTERNAL_VALIDATION_FILE, index_col=0)\n",
    "    print(f\"✅ External CRLM validation set loaded: {dat_crlm.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Error: external validation file not found - {e}\")\n",
    "    raise\n",
    "\n",
    "# Prepare external validation features (align to available_functional_genes)\n",
    "X_val_external = pd.DataFrame(index=dat_crlm.index, columns=available_functional_genes)\n",
    "# If columns exist, copy; otherwise will produce NaN\n",
    "for g in available_functional_genes:\n",
    "    if g in dat_crlm.columns:\n",
    "        X_val_external[g] = dat_crlm[g]\n",
    "    else:\n",
    "        X_val_external[g] = 0.0  # fill missing genes with 0\n",
    "\n",
    "# Prepare external validation labels (convert to binary: metastasis=1 else 0)\n",
    "if 'status' not in dat_crlm.columns:\n",
    "    raise ValueError(\"External validation file must contain a 'status' column.\")\n",
    "y_val_external = dat_crlm['status'].apply(lambda x: 1 if 'metastasis' in str(x).lower() else 0).astype(int)\n",
    "\n",
    "# IMPORTANT: use the scaler fitted on the training set to transform external data\n",
    "X_val_external_scaled = scaler.transform(X_val_external)\n",
    "print(\"✅ External validation data are ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9cafac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Define and evaluate 10 machine learning models ---\n",
    "print(\"\\n--- Step 3: Training and evaluating 10 models ---\")\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=SEED, max_iter=1000),\n",
    "    \"Support Vector Machine\": SVC(probability=True, random_state=SEED),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=SEED),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=SEED),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=SEED),\n",
    "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=SEED),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(random_state=SEED),\n",
    "    \"XGBoost\": xgb.XGBClassifier(random_state=SEED, use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "internal_results = []\n",
    "external_results = []\n",
    "\n",
    "def evaluate_model(y_true, y_pred_proba, y_pred):\n",
    "    auc = roc_auc_score(y_true, y_pred_proba) if len(np.unique(y_true)) > 1 else np.nan\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "    else:\n",
    "        sensitivity = specificity = np.nan\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    return {\n",
    "        \"AUC\": float(auc) if not np.isnan(auc) else np.nan,\n",
    "        \"Accuracy\": float(accuracy),\n",
    "        \"Sensitivity\": float(sensitivity) if not np.isnan(sensitivity) else np.nan,\n",
    "        \"Specificity\": float(specificity) if not np.isnan(specificity) else np.nan,\n",
    "        \"Precision\": float(precision),\n",
    "        \"Recall\": float(recall)\n",
    "    }\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"--- Processing: {name} ---\")\n",
    "    # Fit model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Evaluate on internal test set\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_pred_proba_internal = model.predict_proba(X_test_internal_scaled)[:, 1]\n",
    "    else:\n",
    "        # Fallback: use decision_function and scale to [0,1] via logistic sigmoid\n",
    "        try:\n",
    "            scores = model.decision_function(X_test_internal_scaled)\n",
    "            y_pred_proba_internal = 1 / (1 + np.exp(-scores))\n",
    "        except Exception:\n",
    "            y_pred_proba_internal = np.zeros(len(X_test_internal_scaled))\n",
    "    y_pred_internal = model.predict(X_test_internal_scaled)\n",
    "    res_internal = evaluate_model(y_test_internal, y_pred_proba_internal, y_pred_internal)\n",
    "    res_internal[\"Model\"] = name\n",
    "    internal_results.append(res_internal)\n",
    "\n",
    "    # Evaluate on external validation set\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_pred_proba_external = model.predict_proba(X_val_external_scaled)[:, 1]\n",
    "    else:\n",
    "        try:\n",
    "            scores_ext = model.decision_function(X_val_external_scaled)\n",
    "            y_pred_proba_external = 1 / (1 + np.exp(-scores_ext))\n",
    "        except Exception:\n",
    "            y_pred_proba_external = np.zeros(len(X_val_external_scaled))\n",
    "    y_pred_external = model.predict(X_val_external_scaled)\n",
    "    res_external = evaluate_model(y_val_external, y_pred_proba_external, y_pred_external)\n",
    "    res_external[\"Model\"] = name\n",
    "    external_results.append(res_external)\n",
    "\n",
    "internal_df = pd.DataFrame(internal_results).sort_values(by=\"AUC\", ascending=False).reset_index(drop=True)\n",
    "external_df = pd.DataFrame(external_results).sort_values(by=\"AUC\", ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64da359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Display and save results ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Benchmark results on internal test set\")\n",
    "print(\"=\"*60)\n",
    "print(internal_df)\n",
    "internal_df.to_csv(os.path.join(OUTPUT_DIR, \"internal_test_set_benchmark.csv\"), index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Benchmark results on external CRLM validation set\")\n",
    "print(\"=\"*60)\n",
    "print(external_df)\n",
    "external_df.to_csv(os.path.join(OUTPUT_DIR, \"external_crlm_set_benchmark.csv\"), index=False)\n",
    "print(f\"\\n✅ All results saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3823a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: Visualize results ---\n",
    "print(\"\\n--- Step 5: Visualizing model performance comparisons ---\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 16))\n",
    "fig.suptitle(\"Machine Learning Model Performance Comparison\", fontsize=20, fontweight='bold')\n",
    "\n",
    "# Internal test AUC plot\n",
    "sns.barplot(ax=axes[0], x=\"AUC\", y=\"Model\", data=internal_df, palette=\"Blues_r\")\n",
    "axes[0].set_title(\"AUC on Internal Test Set\", fontsize=16)\n",
    "axes[0].set_xlabel(\"AUC (Area Under Curve)\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Model\", fontsize=12)\n",
    "axes[0].set_xlim(0, 1.0)\n",
    "for index, value in enumerate(internal_df[\"AUC\"]):\n",
    "    axes[0].text(value + 0.01, index, f\"{value:.4f}\", va=\"center\")\n",
    "\n",
    "# External validation AUC plot\n",
    "sns.barplot(ax=axes[1], x=\"AUC\", y=\"Model\", data=external_df, palette=\"Greens_r\")\n",
    "axes[1].set_title(\"AUC on External CRLM Validation Set\", fontsize=16)\n",
    "axes[1].set_xlabel(\"AUC (Area Under Curve)\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Model\", fontsize=12)\n",
    "axes[1].set_xlim(0, 1.0)\n",
    "for index, value in enumerate(external_df[\"AUC\"]):\n",
    "    axes[1].text(value + 0.01, index, f\"{value:.4f}\", va=\"center\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plot_output_file = os.path.join(OUTPUT_DIR, \"benchmark_comparison_plots.png\")\n",
    "plt.savefig(plot_output_file, dpi=300)\n",
    "print(f\"✅ Performance comparison plot saved to: {plot_output_file}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Advanced benchmark completed ---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
